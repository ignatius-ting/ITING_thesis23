{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ignatius Ting\\AppData\\Local\\Temp\\ipykernel_22148\\456950064.py:5: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/counts_1m_trim.pkl', 'rb') as f:\n",
    "    counts_1m = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247160, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_1m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe df1:\n",
      "Shape: (49432, 6)\n",
      "Shape after join: (4476067, 10)\n",
      "Dataframe df2:\n",
      "Shape: (49432, 6)\n",
      "Shape after join: (4476641, 10)\n",
      "Dataframe df3:\n",
      "Shape: (49432, 6)\n",
      "Shape after join: (4543464, 10)\n",
      "Dataframe df4:\n",
      "Shape: (49432, 6)\n",
      "Shape after join: (4374090, 10)\n",
      "Dataframe df5:\n",
      "Shape: (49432, 6)\n",
      "Shape after join: (4435315, 10)\n"
     ]
    }
   ],
   "source": [
    "large_df = counts_1m[['source_cui','source_text','rel','cui_2','cui_2_text','rel_type']].copy()\n",
    "\n",
    "# Randomly shuffle dataframe\n",
    "large_df = large_df.sample(frac = 1, random_state = 150).reset_index(drop = True)\n",
    "num_chunks = 5\n",
    "\n",
    "# Calculate the size of each smaller dataframe\n",
    "chunk_size = len(large_df) // num_chunks\n",
    "\n",
    "# Create a dictionary to store the smaller dataframes\n",
    "small_dataframes = {}\n",
    "\n",
    "#2nd hop DF\n",
    "hop2 = counts_1m[counts_1m.rel_type == 'RO'][['source_cui','rel','cui_2','cui_2_text']].copy()\n",
    "hop2.columns = ['cui_2','rel_2','cui_3','cui_3_text']\n",
    "\n",
    "\n",
    "# Split the large dataframe into smaller dataframes and store them in the dictionary\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = (i + 1) * chunk_size if i < num_chunks - 1 else len(large_df)\n",
    "    small_dataframes[f'df{i+1}'] = large_df.iloc[start_idx:end_idx]\n",
    "\n",
    "# Access the smaller dataframes from the dictionary\n",
    "for i in range(5):\n",
    "    df_name = f'df{i+1}'\n",
    "    small_df = small_dataframes[df_name]\n",
    "    print(f\"Dataframe {df_name}:\")\n",
    "    print(f\"Shape: {small_df.shape}\")\n",
    "    small_df = small_df.merge(hop2, on = 'cui_2')\n",
    "    # 1st and 3rd node cannot be the same\n",
    "    small_df = small_df[small_df.source_cui != small_df.cui_3]\n",
    "    # print(small_df.columns)\n",
    "    small_df['whole_text'] = small_df.apply(lambda x: ' '.join([str(x['source_text']),\n",
    "                                                            str(x['rel']),\n",
    "                                                            str(x['cui_2_text']),\n",
    "                                                            str(x['rel_2']),\n",
    "                                                            str(x['cui_3_text'])]), axis=1)\n",
    "    small_dataframes[df_name] = small_df\n",
    "    print(f\"Shape after join: {small_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples from batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to randomly select 3 rows from each group\n",
    "def select_random_rows(group):\n",
    "    if len(group) >= 3:\n",
    "        return group.sample(3, random_state=42)  # You can change the random_state for different random selections\n",
    "    else:\n",
    "        return group\n",
    "df = pd.DataFrame()\n",
    "for i in range(1,6):\n",
    "    df_key = f\"df{i}\"\n",
    "    df1 = small_dataframes[df_key].copy()\n",
    "    # Group by the column containing duplicates and apply the selection function\n",
    "    df1_1 = df1.groupby('source_cui', group_keys=False).apply(select_random_rows)\n",
    "    df1_1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df1_2 = df1.groupby('cui_2', group_keys=False).apply(select_random_rows)\n",
    "    df1_2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df1_3 = df1.groupby('cui_3', group_keys=False).apply(select_random_rows)\n",
    "    df1_3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df1 = pd.concat([df1_1,df1_2,df1_3])\n",
    "    df1.drop_duplicates('whole_text', keep = 'first', inplace = True)\n",
    "    df1.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    df = pd.concat([df, df1], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1255340, 10)\n",
      "(1255340, 10)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df = df.drop_duplicates('whole_text', keep = 'first')\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = df.loc[:418446]\n",
    "sample_2 = df.loc[418446:836892]\n",
    "sample_3 = df.loc[836892:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1.to_csv(\"../data/1mm_even_batches/1mm_even_1.csv\", index = False)\n",
    "sample_2.to_csv(\"../data/1mm_even_batches/1mm_even_2.csv\", index = False)\n",
    "sample_3.to_csv(\"../data/1mm_even_batches/1mm_even_3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = small_dataframes['df2']\n",
    "\n",
    "# sample_1 = df.sample(30000, random_state = 110)\n",
    "# sample_2 = df[~df.index.isin(sample_1.index)].sample(30000, random_state = 111)\n",
    "# indices = list(sample_1.index) + list(sample_2.index)\n",
    "# sample_3 = df[~df.index.isin(indices)].sample(30000, random_state = 112)\n",
    "# indices = list(sample_1.index) + list(sample_2.index) + list(sample_3.index)\n",
    "# sample_4 = df[~df.index.isin(indices)].sample(30000, random_state = 113)\n",
    "\n",
    "# sample_1.to_csv('../data/1mm_final_trim/1mm_batch_2_1.csv', index = False)\n",
    "# sample_2.to_csv('../data/1mm_final_trim/1mm_batch_2_2.csv', index = False)\n",
    "# sample_3.to_csv('../data/1mm_final_trim/1mm_batch_2_3.csv', index = False)\n",
    "# sample_4.to_csv('../data/1mm_final_trim/1mm_batch_2_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples from batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = small_dataframes['df3']\n",
    "\n",
    "sample_1 = df.sample(50000, random_state = 110)\n",
    "sample_2 = df[~df.index.isin(sample_1.index)].sample(50000, random_state = 111)\n",
    "indices = list(sample_1.index) + list(sample_2.index)\n",
    "sample_3 = df[~df.index.isin(indices)].sample(50000, random_state = 112)\n",
    "indices = list(sample_1.index) + list(sample_2.index) + list(sample_3.index)\n",
    "sample_4 = df[~df.index.isin(indices)].sample(50000, random_state = 113)\n",
    "\n",
    "sample_1.to_csv('../data/1mm_final_trim/1mm_batch_3_1.csv', index = False)\n",
    "sample_2.to_csv('../data/1mm_final_trim/1mm_batch_3_2.csv', index = False)\n",
    "sample_3.to_csv('../data/1mm_final_trim/1mm_batch_3_3.csv', index = False)\n",
    "sample_4.to_csv('../data/1mm_final_trim/1mm_batch_3_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/1mm_batches/1mm_batch_df2.csv')\n",
    "\n",
    "# sample_1 = df.sample(12000, random_state = 123)\n",
    "# sample_2 = df[~df.index.isin(sample_1.index)].sample(12000, random_state = 123)\n",
    "# indices = list(sample_1.index) + list(sample_2.index)\n",
    "# sample_3 = df[~df.index.isin(indices)].sample(12000, random_state = 123)\n",
    "\n",
    "# sample_1.to_csv('../data/1mm_batches/sub_batch_1/1mm_batch_df2_1.csv', index = False)\n",
    "# sample_2.to_csv('../data/1mm_batches/sub_batch_1/1mm_batch_df2_2.csv', index = False)\n",
    "# sample_3.to_csv('../data/1mm_batches/sub_batch_1/1mm_batch_df2_3.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/1mm_batches/1mm_batch_df3.csv')\n",
    "# df = df[(df.rel != 'do not code with') & (df.rel != 'do not code with')].reset_index(drop = True)\n",
    "\n",
    "# sample_1 = df.sample(30000, random_state = 123)\n",
    "# sample_2 = df[~df.index.isin(sample_1.index)].sample(30000, random_state = 123)\n",
    "# indices = list(sample_1.index) + list(sample_2.index)\n",
    "# sample_3 = df[~df.index.isin(indices)].sample(30000, random_state = 123)\n",
    "\n",
    "# sample_1.to_csv('../data/1mm_batches/sub_batch_1/1mm_batch_df3_1.csv', index = False)\n",
    "# sample_2.to_csv('../data/1mm_batches/sub_batch_1/1mm_batch_df3_2.csv', index = False)\n",
    "# sample_3.to_csv('../data/1mm_batches/sub_batch_1/1mm_batch_df3_3.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
