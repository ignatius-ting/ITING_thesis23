{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ignatius Ting\\AppData\\Local\\Temp\\ipykernel_28032\\405212744.py:5: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('max_colwidth', -1)\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ignatius\n",
      "[nltk_data]     Ting\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Ignatius\n",
      "[nltk_data]     Ting\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/counts_1m.pkl', 'rb') as f:\n",
    "    counts_1m = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary columns and most frequent concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_cols = ['mapped from',\n",
    "'mapped to',\n",
    "'do not code with',\n",
    "'associated with',\n",
    "'interprets',\n",
    "'is interpreted by',\n",
    "'interpretation of',\n",
    "'has interpretation'\n",
    "'other mapped from',\n",
    "'other mapped to',\n",
    "'inactive ingredient of',\n",
    "'has inactive ingredient'\n",
    "]\n",
    "counts_1m = counts_1m[~counts_1m.rel.isin(rm_cols)]\n",
    "counts_1m = counts_1m[(counts_1m.source_text != 'EXCISION') & (counts_1m.cui_2_text != 'EXCISION')].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "EN_STOPWORDS = list(set(stopwords.words('english')))\n",
    "\n",
    "def count_non_stop_words(text):\n",
    "    # Tokenize the text\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Get the list of English stop words\n",
    "    stop_words = EN_STOPWORDS\n",
    "    \n",
    "    # Filter out the stop words and count non-stop words\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "    return len(filtered_words)\n",
    "    # return filtered_words\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = EN_STOPWORDS\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "    return set(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of non-stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counts_1m['source_true_len'] = counts_1m.source_text.apply(lambda x: count_non_stop_words(x))\n",
    "counts_1m['cui_2_true_len'] = counts_1m.cui_2_text.apply(lambda x: count_non_stop_words(x))\n",
    "# filter for only 5 or less\n",
    "# counts_1m = counts_1m[(counts_1m.source_true_len <= 5) & (counts_1m.cui_2_true_len <= 5)]\n",
    "\n",
    "# Apply the function to each row in both columns\n",
    "counts_1m['source_text_words'] = counts_1m['source_text'].apply(remove_stopwords)\n",
    "counts_1m['cui_2_text_words'] = counts_1m['cui_2_text'].apply(remove_stopwords)\n",
    "\n",
    "# Calculate the number of overlapping words\n",
    "counts_1m['overlap_count'] = counts_1m.apply(lambda row: len(row['source_text_words'].intersection(row['cui_2_text_words'])), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count overlapping substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def largest_overlapping_substring(str1, str2):\n",
    "    # some preprocessing\n",
    "    str1 = str1.lower()\n",
    "    str2 = str2.lower()\n",
    "    str1 = re.sub(r'[^\\w\\s]', '', str1)\n",
    "    str2 = re.sub(r'[^\\w\\s]', '', str2)\n",
    "    # Initialize a matrix to store the lengths of overlapping substrings\n",
    "    matrix = [[0] * (len(str2) + 1) for _ in range(len(str1) + 1)]\n",
    "\n",
    "    max_length = 0  # Maximum overlapping substring length\n",
    "    end_index = 0   # Ending index of the maximum overlapping substring\n",
    "    \n",
    "    for i in range(1, len(str1) + 1):\n",
    "        for j in range(1, len(str2) + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                matrix[i][j] = matrix[i - 1][j - 1] + 1\n",
    "                if matrix[i][j] > max_length:\n",
    "                    max_length = matrix[i][j]\n",
    "                    end_index = i  # Update the ending index\n",
    "    \n",
    "    # Extract the largest overlapping substring\n",
    "    largest_overlap = str1[end_index - max_length:end_index]\n",
    "    \n",
    "    return largest_overlap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row in the DataFrame\n",
    "counts_1m['largest_overlap'] = counts_1m.apply(lambda row: largest_overlapping_substring(row['source_text'], row['cui_2_text']), axis=1)\n",
    "\n",
    "# Compare the length of the largest overlapping substring to half the length of the shorter string\n",
    "counts_1m['overlap_pass'] = counts_1m.apply(lambda row: len(row['largest_overlap']) > min(len(row['source_text']), len(row['cui_2_text'])) / 2, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>688610</th>\n",
       "      <td>puncture of left wrist with surrounding tenderness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239701</th>\n",
       "      <td>Left lobe of liver, NOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752436</th>\n",
       "      <td>FORSYTHE-WAKELING SYNDROME</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               source_text\n",
       "688610  puncture of left wrist with surrounding tenderness\n",
       "239701  Left lobe of liver, NOS                           \n",
       "752436  FORSYTHE-WAKELING SYNDROME                        "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_1m[['source_text']].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_1m = counts_1m[(counts_1m.source_true_len <= 5) & (counts_1m.cui_2_true_len <= 5)]\n",
    "counts_1m = counts_1m[counts_1m.overlap_count < 3]\n",
    "counts_1m = counts_1m[counts_1m.overlap_pass == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_1m.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/counts_1m_trim.pkl', 'wb') as file:\n",
    "    pickle.dump(counts_1m, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
