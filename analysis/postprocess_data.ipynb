{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unidecode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point(n):\n",
    "    n = n.strip()\n",
    "    n = n.replace('(', '')\n",
    "    n = n.replace('\\\"', '')\n",
    "    n = n.replace(')', '')\n",
    "    n = n.replace(',', ' ')\n",
    "    n = n.replace('_', ' ')\n",
    "    n = n.replace('  ',' ')\n",
    "    n = unidecode.unidecode(n)\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../data/processed/final_2hop_v1.csv')\n",
    "df3 = pd.read_csv('../data/processed/combined_3hop_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['source_clean'] = df2.source_text.apply(lambda x: get_point(x))\n",
    "df2['cui_2_clean'] = df2.cui_2_text.apply(lambda x: get_point(x))\n",
    "df2['cui_3_clean'] = df2.cui_3_text.apply(lambda x: get_point(x))\n",
    "\n",
    "df2['rel_1_clean'] = df2['relation 1'].apply(lambda x: get_point(x))\n",
    "df2['rel_2_clean'] = df2['relation 2'].apply(lambda x: get_point(x))\n",
    "df2['rel_combine'] = df2.apply(lambda x: ' '.join([x['rel_1_clean'], x['rel_2_clean']]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['source'] = df2.apply(lambda x: f\"[['{x['source_clean']}', '{x['rel_1_clean']}', '{x['cui_2_clean']}'], ['{x['cui_2_clean']}', '{x['rel_2_clean']}', '{x['cui_3_clean']}']]\", axis = 1)\n",
    "df2['target'] = df2.apply(lambda x: f\"{x['result']}\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['source_clean'] = df3.source_text.apply(lambda x: get_point(x))\n",
    "df3['cui_2_clean'] = df3.cui_2_text.apply(lambda x: get_point(x))\n",
    "df3['cui_3_clean'] = df3.cui_3_text.apply(lambda x: get_point(x))\n",
    "df3['cui_4_clean'] = df3.cui_4_text.apply(lambda x: get_point(x))\n",
    "\n",
    "df3['rel_1_clean'] = df3['relation'].apply(lambda x: get_point(x))\n",
    "df3['rel_2_clean'] = df3['relation_2'].apply(lambda x: get_point(x))\n",
    "df3['rel_3_clean'] = df3['relation_3'].apply(lambda x: get_point(x))\n",
    "df3['rel_combine'] = df3.apply(lambda x: ' '.join([x['rel_1_clean'], \n",
    "                                                   x['rel_2_clean'],\n",
    "                                                   x['rel_3_clean']]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['source'] = df3.apply(lambda x: f\"[['{x['source_clean']}', '{x['rel_1_clean']}', '{x['cui_2_clean']}'], ['{x['cui_2_clean']}', '{x['rel_2_clean']}', '{x['cui_3_clean']}'], ['{x['cui_3_clean']}', '{x['rel_3_clean']}', '{x['cui_4_clean']}']]\", axis = 1)\n",
    "df3['target'] = df3.apply(lambda x: f\"{x['result']}\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which can be stratified\n",
    "split_dict = dict(df2.rel_combine.value_counts() > 1)\n",
    "df2['strat_split'] = df2.rel_combine.apply(lambda x: split_dict[x])\n",
    "\n",
    "# separate dfs\n",
    "df_str = df2[df2.strat_split == True].reset_index(drop = True)\n",
    "df_nostr = df2[df2.strat_split == False].reset_index(drop = True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Specify the feature (topic) column and the target variable (text) column.\n",
    "feature_column = 'rel_combine'\n",
    "text_column = 'result'\n",
    "\n",
    "# Adjust the test_size and random_state as needed.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df_str[text_column],\n",
    "    df_str[feature_column],\n",
    "    test_size=0.2,  # Adjust the test size as needed.\n",
    "    stratify=df_str[feature_column], # stratify by column\n",
    "    random_state=42  # Adjust the random seed as needed.\n",
    ")\n",
    "\n",
    "# Split the remaining data into validation and testing sets (50% each).\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42  # Use the same random seed for consistency.\n",
    ")\n",
    "\n",
    "df_train1 = df_str[df_str.index.isin(X_train.index)]\n",
    "df_val1 = df_str[df_str.index.isin(X_val.index)]\n",
    "df_test1 = df_str[df_str.index.isin(X_test.index)]\n",
    "\n",
    "# Adjust the test_size and random_state as needed.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df_nostr[text_column],\n",
    "    df_nostr[feature_column],\n",
    "    test_size=0.2,  # Adjust the test size as needed.\n",
    "    random_state=42  # Adjust the random seed as needed.\n",
    ")\n",
    "\n",
    "# Split the remaining data into validation and testing sets (50% each).\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42  # Use the same random seed for consistency.\n",
    ")\n",
    "\n",
    "df_train2 = df_nostr[df_nostr.index.isin(X_train.index)]\n",
    "df_val2 = df_nostr[df_nostr.index.isin(X_val.index)]\n",
    "df_test2 = df_nostr[df_nostr.index.isin(X_test.index)]\n",
    "\n",
    "df2_train = pd.concat([df_train1,df_train2])\n",
    "df2_test = pd.concat([df_test1,df_test2])\n",
    "df2_val = pd.concat([df_val1,df_val2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which can be stratified\n",
    "split_dict = dict(df3.rel_combine.value_counts() > 1)\n",
    "df3['strat_split'] = df3.rel_combine.apply(lambda x: split_dict[x])\n",
    "\n",
    "# separate dfs\n",
    "df_str = df3[df3.strat_split == True].reset_index(drop = True)\n",
    "df_nostr = df3[df3.strat_split == False].reset_index(drop = True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Specify the feature (topic) column and the target variable (text) column.\n",
    "feature_column = 'rel_combine'\n",
    "text_column = 'result'\n",
    "\n",
    "# Adjust the test_size and random_state as needed.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df_str[text_column],\n",
    "    df_str[feature_column],\n",
    "    test_size=0.2,  # Adjust the test size as needed.\n",
    "    stratify=df_str[feature_column], # stratify by column\n",
    "    random_state=42  # Adjust the random seed as needed.\n",
    ")\n",
    "\n",
    "# Split the remaining data into validation and testing sets (50% each).\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42  # Use the same random seed for consistency.\n",
    ")\n",
    "\n",
    "df_train1 = df_str[df_str.index.isin(X_train.index)]\n",
    "df_val1 = df_str[df_str.index.isin(X_val.index)]\n",
    "df_test1 = df_str[df_str.index.isin(X_test.index)]\n",
    "\n",
    "# Adjust the test_size and random_state as needed.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df_nostr[text_column],\n",
    "    df_nostr[feature_column],\n",
    "    test_size=0.2,  # Adjust the test size as needed.\n",
    "    random_state=42  # Adjust the random seed as needed.\n",
    ")\n",
    "\n",
    "# Split the remaining data into validation and testing sets (50% each).\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42  # Use the same random seed for consistency.\n",
    ")\n",
    "\n",
    "df_train2 = df_nostr[df_nostr.index.isin(X_train.index)]\n",
    "df_val2 = df_nostr[df_nostr.index.isin(X_val.index)]\n",
    "df_test2 = df_nostr[df_nostr.index.isin(X_test.index)]\n",
    "\n",
    "df3_train = pd.concat([df_train1,df_train2])\n",
    "df3_test = pd.concat([df_test1,df_test2])\n",
    "df3_val = pd.concat([df_val1,df_val2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df2_train, df3_train])\n",
    "df_val = pd.concat([df2_val, df3_val])\n",
    "df_test = pd.concat([df2_test, df3_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "train_json = df_train.reset_index()[['source','target']]\n",
    "val_json = df_val.reset_index()[['source','target']]\n",
    "test_json = df_test.reset_index()[['source','target']]\n",
    "\n",
    "data_dir = '../data/modelling/'\n",
    "\n",
    "train_file = os.path.join(data_dir,'train_g2t_v1.json')\n",
    "val_file = os.path.join(data_dir,'val_g2t_v1.json')\n",
    "test_file = os.path.join(data_dir,'test_g2t_v1.json')\n",
    "\n",
    "\n",
    "with open(train_file, 'w', encoding = 'utf-8') as train_json_file:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in train_json.iterrows():\n",
    "        # Convert the row to a dictionary and write it to the file\n",
    "        json.dump(row.to_dict(), train_json_file, ensure_ascii = False)\n",
    "        # Add a newline character to separate the JSON objects\n",
    "        train_json_file.write('\\n')\n",
    "\n",
    "with open(val_file, 'w', encoding = 'utf-8') as val_json_file:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in val_json.iterrows():\n",
    "        # Convert the row to a dictionary and write it to the file\n",
    "        json.dump(row.to_dict(), val_json_file, ensure_ascii = False)\n",
    "        # Add a newline character to separate the JSON objects\n",
    "        val_json_file.write('\\n')\n",
    "\n",
    "with open(test_file, 'w', encoding = 'utf-8') as test_json_file:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in test_json.iterrows():\n",
    "        # Convert the row to a dictionary and write it to the file\n",
    "        json.dump(row.to_dict(), test_json_file, ensure_ascii = False)\n",
    "        # Add a newline character to separate the JSON objects\n",
    "        test_json_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../data/modelling/train_g2t_v1.csv', index = False)\n",
    "df_test.to_csv('../data/modelling/val_g2t_v1.csv', index = False)\n",
    "df_val.to_csv('../data/modelling/test_g2t_v1.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
