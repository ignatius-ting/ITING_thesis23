{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ignatius Ting\\AppData\\Local\\Temp\\ipykernel_9600\\3472114568.py:6: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = pd.read_json('../data/modelling_enc/rel_val_v1.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feature_rel', 'target_rel'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zz.target_rel.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = pd.read_json('../data/modelling_enc/rel_train_v1.json', lines = True)\n",
    "len(aa.target_rel.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30543, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/modelling/v2_data/df_train_v2.csv')\n",
    "test = pd.read_csv('../data/modelling/v2_data/df_test_v2.csv')\n",
    "val = pd.read_csv('../data/modelling/v2_data/df_val_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train,test,val])\n",
    "df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "df['target'] = df.apply(lambda x: [x['rel_1_clean'], x['rel_2_clean']] if pd.isna(x['rel_3_clean'])\\\n",
    "                              else [x['rel_1_clean'], x['rel_2_clean'], x['rel_3_clean']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25116, 27)\n",
      "(22777, 29)\n"
     ]
    }
   ],
   "source": [
    "# get adequate class distribution\n",
    "temp = df.target.value_counts()\n",
    "targets = temp[temp > 4].index.tolist()\n",
    "print(df.shape)\n",
    "df = df[df.target.isin(targets)].reset_index(drop = True)\n",
    "print(zz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triples(row):\n",
    "    triples = []\n",
    "    \n",
    "    for i in range(1,4):\n",
    "        if i == 1:\n",
    "            source_col = row[f'source_text']\n",
    "        else:\n",
    "            source_col = row[f'cui_{i}_text']\n",
    "        cui_col = row[f'cui_{i+1}_text']\n",
    "        rel_col = row[f'rel_{i}_clean']\n",
    "        \n",
    "        if not pd.isna(rel_col):\n",
    "            triples.append((source_col, rel_col, cui_col))\n",
    "    \n",
    "    return triples\n",
    "\n",
    "df['triples'] = df.apply(create_triples, axis=1)\n",
    "df['triples'] = df['triples'].apply(lambda x: [triple for triple in x if triple])  # Remove empty lists\n",
    "df['triples'] = df['triples'].apply(lambda x: tuple(x))\n",
    "df_t5 = df.copy()\n",
    "# print(df[['source_text', 'triples']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['explode_key'] = df['triples'].copy()\n",
    "df_rel = df.explode('explode_key')\n",
    "for x in df_rel.explode_key:\n",
    "    assert len(x) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50901, 32)\n",
      "(50901, 32)\n"
     ]
    }
   ],
   "source": [
    "df_rel['target_rel'] = df_rel.explode_key.apply(lambda x: x[1])\n",
    "df_rel['entities'] = df_rel.explode_key.apply(lambda x: [x[0],x[2]])\n",
    "df_rel['feature_rel'] = df_rel.apply(lambda x: f\"{x['entities']} [SEP] {x['result']}\", axis = 1)\n",
    "\n",
    "\n",
    "temp = df_rel.target_rel.value_counts()\n",
    "targets = temp[temp > 4].index.tolist()\n",
    "print(df_rel.shape)\n",
    "# get triples of relations that fall under threshold\n",
    "triples_rm = df_rel[~df_rel.target_rel.isin(targets)].triples.unique()\n",
    "# # Convert the inner lists to tuples\n",
    "# unique_tuples = {tuple(inner_list) for sublist in triples_rm for inner_list in sublist}\n",
    "\n",
    "# # Convert the unique tuples back to lists\n",
    "# triples_rm = [list(inner_tuple) for inner_tuple in unique_tuples]\n",
    "df_rel = df_rel[~df_rel.triples.isin(triples_rm)]\n",
    "print(df_rel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't want to spread instances of the same triple across train/val/test\n",
    "# take one instance of each triple set, and then do split\n",
    "# then we can rejoin the individual rel targets after\n",
    "df_rel_split = df_rel.drop_duplicates('triples', keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def custom_train_test_validation_split(df, train_ratio=0.6, test_ratio=0.2, validation_ratio=0.2, random_state=None):\n",
    "    # First, split the data into training and temporary (test + validation) sets\n",
    "    df_train, df_temp = train_test_split(df, test_size=(test_ratio + validation_ratio), random_state=random_state, stratify=df['target'])\n",
    "    \n",
    "    # Then, split the temporary set into test and validation sets\n",
    "    relative_validation_ratio = validation_ratio / (test_ratio + validation_ratio)\n",
    "    df_test, df_val = train_test_split(df_temp, test_size=relative_validation_ratio, random_state=random_state, stratify=df_temp['target'])\n",
    "    \n",
    "    return df_train, df_test, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_train, rel_test, rel_val = custom_train_test_validation_split(df = df_rel_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13665, 32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source_cui', 'relation 1', 'cui_2', 'relation 2', 'cui_3', 'query',\n",
       "       'url', 'result', 'source_text', 'cui_2_text', 'cui_3_text',\n",
       "       'source_clean', 'cui_2_clean', 'cui_3_clean', 'rel_1_clean',\n",
       "       'rel_2_clean', 'rel_combine', 'source', 'target', 'strat_split',\n",
       "       'relation', 'relation_2', 'relation_3', 'cui_4', 'cui_4_text',\n",
       "       'cui_4_clean', 'rel_3_clean', 'triples', 'explode_key', 'target_rel',\n",
       "       'entities', 'feature_rel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_train = df_rel.merge(rel_train[['triples']], on = 'triples', how = 'inner')\n",
    "rel_test = df_rel.merge(rel_test[['triples']], on = 'triples', how = 'inner')\n",
    "rel_val = df_rel.merge(rel_val[['triples']], on = 'triples', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source_cui', 'relation 1', 'cui_2', 'relation 2', 'cui_3', 'query',\n",
       "       'url', 'result', 'source_text', 'cui_2_text', 'cui_3_text',\n",
       "       'source_clean', 'cui_2_clean', 'cui_3_clean', 'rel_1_clean',\n",
       "       'rel_2_clean', 'rel_combine', 'source', 'target', 'strat_split',\n",
       "       'relation', 'relation_2', 'relation_3', 'cui_4', 'cui_4_text',\n",
       "       'cui_4_clean', 'rel_3_clean', 'triples', 'explode_key', 'target_rel',\n",
       "       'entities', 'feature_rel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def write_df_to_json(dataframe, filename, data_dir='../data/', encoding='utf-8'):\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "    with open(file_path, 'w', encoding=encoding) as json_file:\n",
    "        # Iterate over the rows of the DataFrame\n",
    "        for _, row in dataframe.iterrows():\n",
    "            # Convert the row to a dictionary and write it to the file\n",
    "            json.dump(row.to_dict(), json_file, ensure_ascii=False)\n",
    "            # Add a newline character to separate the JSON objects\n",
    "            json_file.write('\\n')\n",
    "# Example usage:\n",
    "write_df_to_json(rel_train[['feature_rel','target_rel']], 'rel_train_v1.json', data_dir = '../data/modelling_enc/')\n",
    "write_df_to_json(rel_val[['feature_rel','target_rel']], 'rel_val_v1.json', data_dir = '../data/modelling_enc/')\n",
    "write_df_to_json(rel_test[['feature_rel','target_rel']], 'rel_test_v1.json', data_dir = '../data/modelling_enc/')\n",
    "\n",
    "write_df_to_json(rel_train, 'full_rel_train_v1.json', data_dir = '../data/modelling_enc/')\n",
    "write_df_to_json(rel_val, 'full_val_v1.json', data_dir = '../data/modelling_enc/')\n",
    "write_df_to_json(rel_test, 'full_test_v1.json', data_dir = '../data/modelling_enc/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy for encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_t5 = df_train.copy()\n",
    "df_test_t5 = df_test.copy()\n",
    "df_val_t5 = df_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['feature'] = df_train['source'].apply(lambda x: x.replace(\"</s>\", \"[SEP]\"))\n",
    "df_test['feature'] = df_test['source'].apply(lambda x: x.replace(\"</s>\", \"[SEP]\"))\n",
    "df_val['feature'] = df_val['source'].apply(lambda x: x.replace(\"</s>\", \"[SEP]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write encoder data to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "train_json = df_train[['feature','target']]\n",
    "val_json = df_val[['feature','target']]\n",
    "test_json = df_test[['feature','target']]\n",
    "\n",
    "data_dir = '../data/modelling_enc/'\n",
    "\n",
    "train_file = os.path.join(data_dir,'train_v3.json')\n",
    "val_file = os.path.join(data_dir,'val_v3.json')\n",
    "test_file = os.path.join(data_dir,'test_v3.json')\n",
    "\n",
    "\n",
    "with open(train_file, 'w', encoding = 'utf-8') as train_json_file:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in train_json.iterrows():\n",
    "        # Convert the row to a dictionary and write it to the file\n",
    "        json.dump(row.to_dict(), train_json_file, ensure_ascii = False)\n",
    "        # Add a newline character to separate the JSON objects\n",
    "        train_json_file.write('\\n')\n",
    "\n",
    "with open(val_file, 'w', encoding = 'utf-8') as val_json_file:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in val_json.iterrows():\n",
    "        # Convert the row to a dictionary and write it to the file\n",
    "        json.dump(row.to_dict(), val_json_file, ensure_ascii = False)\n",
    "        # Add a newline character to separate the JSON objects\n",
    "        val_json_file.write('\\n')\n",
    "\n",
    "with open(test_file, 'w', encoding = 'utf-8') as test_json_file:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in test_json.iterrows():\n",
    "        # Convert the row to a dictionary and write it to the file\n",
    "        json.dump(row.to_dict(), test_json_file, ensure_ascii = False)\n",
    "        # Add a newline character to separate the JSON objects\n",
    "        test_json_file.write('\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_t5.rename(columns = {'source':'feature'}, inplace = True)\n",
    "df_test_t5.rename(columns = {'source':'feature'}, inplace = True)\n",
    "df_val_t5.rename(columns = {'source':'feature'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_t5['target'] = df_train_t5.apply(lambda x: f\"[['{x['source_clean']}', '{x['rel_1_clean']}', '{x['cui_2_clean']}'], ['{x['cui_2_clean']}', '{x['rel_2_clean']}', '{x['cui_3_clean']}']]\" \\\n",
    "                        if pd.isna(x['rel_3_clean']) else\\\n",
    "                        f\"[['{x['source_clean']}', '{x['rel_1_clean']}', '{x['cui_2_clean']}'], ['{x['cui_2_clean']}', '{x['rel_2_clean']}', '{x['cui_3_clean']}'], ['{x['cui_3_clean']}', '{x['rel_3_clean']}', '{x['cui_4_clean']}']]\", \n",
    "                        axis = 1)\n",
    "df_test_t5['target'] = df_test_t5.apply(lambda x: f\"[['{x['source_clean']}', '{x['rel_1_clean']}', '{x['cui_2_clean']}'], ['{x['cui_2_clean']}', '{x['rel_2_clean']}', '{x['cui_3_clean']}']]\" \\\n",
    "                        if pd.isna(x['rel_3_clean']) else\\\n",
    "                        f\"[['{x['source_clean']}', '{x['rel_1_clean']}', '{x['cui_2_clean']}'], ['{x['cui_2_clean']}', '{x['rel_2_clean']}', '{x['cui_3_clean']}'], ['{x['cui_3_clean']}', '{x['rel_3_clean']}', '{x['cui_4_clean']}']]\", \n",
    "                        axis = 1)\n",
    "df_val_t5['target'] = df_val_t5.apply(lambda x: f\"[['{x['source_clean']}', '{x['rel_1_clean']}', '{x['cui_2_clean']}'], ['{x['cui_2_clean']}', '{x['rel_2_clean']}', '{x['cui_3_clean']}']]\" \\\n",
    "                        if pd.isna(x['rel_3_clean']) else\\\n",
    "                        f\"[['{x['source_clean']}', '{x['rel_1_clean']}', '{x['cui_2_clean']}'], ['{x['cui_2_clean']}', '{x['rel_2_clean']}', '{x['cui_3_clean']}'], ['{x['cui_3_clean']}', '{x['rel_3_clean']}', '{x['cui_4_clean']}']]\", \n",
    "                        axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11105</th>\n",
       "      <td>[has finding site, finding site of]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    target\n",
       "11105  [has finding site, finding site of]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['target']].sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_t5.to_csv('../data/modelling/train_v3.csv', index = False)\n",
    "df_test_t5.to_csv('../data/modelling/test_v3.csv', index = False)\n",
    "df_val_t5.to_csv('../data/modelling/val_v3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13666, 27)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_t5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_t5.feature.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13666, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.feature.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "train_json = df_train_t5[['feature','target']]\n",
    "val_json = df_test_t5[['feature','target']]\n",
    "test_json = df_val_t5[['feature','target']]\n",
    "\n",
    "data_dir = '../data/modelling/'\n",
    "\n",
    "train_file = os.path.join(data_dir,'train_v3.json')\n",
    "val_file = os.path.join(data_dir,'val_v3.json')\n",
    "test_file = os.path.join(data_dir,'test_v3.json')\n",
    "\n",
    "\n",
    "with open(train_file, 'w', encoding = 'utf-8') as train_json_file:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in train_json.iterrows():\n",
    "        # Convert the row to a dictionary and write it to the file\n",
    "        json.dump(row.to_dict(), train_json_file, ensure_ascii = False)\n",
    "        # Add a newline character to separate the JSON objects\n",
    "        train_json_file.write('\\n')\n",
    "\n",
    "with open(val_file, 'w', encoding = 'utf-8') as val_json_file:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in val_json.iterrows():\n",
    "        # Convert the row to a dictionary and write it to the file\n",
    "        json.dump(row.to_dict(), val_json_file, ensure_ascii = False)\n",
    "        # Add a newline character to separate the JSON objects\n",
    "        val_json_file.write('\\n')\n",
    "\n",
    "with open(test_file, 'w', encoding = 'utf-8') as test_json_file:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in test_json.iterrows():\n",
    "        # Convert the row to a dictionary and write it to the file\n",
    "        json.dump(row.to_dict(), test_json_file, ensure_ascii = False)\n",
    "        # Add a newline character to separate the JSON objects\n",
    "        test_json_file.write('\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
