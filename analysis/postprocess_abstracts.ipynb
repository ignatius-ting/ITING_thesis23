{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.expand_frame_repr', False)\n",
    "# pd.set_option('max_colwidth', -1)\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from tqdm import tqdm\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/combined_1mm_titles_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/all_dicts.pkl', 'rb') as f:\n",
    "    all_dicts = pickle.load(f)\n",
    "desc_dict = all_dicts['desc_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source_text'] = df.source_cui.apply(lambda x: desc_dict[x])\n",
    "df['cui_2_text'] = df.cui_2.apply(lambda x: desc_dict[x])\n",
    "df['cui_3_text'] = df.cui_3.apply(lambda x: desc_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode \n",
    "def get_point(n):\n",
    "    n = n.strip()\n",
    "    n = n.replace('(', '')\n",
    "    n = n.replace('\\\"', '')\n",
    "    n = n.replace(')', '')\n",
    "    n = n.replace(',', ' ')\n",
    "    n = n.replace('_', ' ')\n",
    "    n = n.replace('  ',' ')\n",
    "    n = unidecode.unidecode(n)\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source_clean'] = df.source_text.apply(lambda x: get_point(x))\n",
    "df['cui_2_clean'] = df.cui_2_text.apply(lambda x: get_point(x))\n",
    "df['cui_3_clean'] = df.cui_3_text.apply(lambda x: get_point(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = df.apply(lambda x: f\"['{x['source_clean']}', '{x['cui_2_clean']}','{x['cui_3_clean']}'] </s> {x['result']}\", axis = 1)\n",
    "df['target'] = df.apply(lambda x: f\"[['{x['source_clean']}', '{x['relation 1']}', '{x['cui_2_clean']}'], ['{x['cui_2_clean']}', '{x['relation 2']}', '{x['cui_3_clean']}']]\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rel_1_clean'] = df['relation 1'].apply(lambda x: get_point(x))\n",
    "df['rel_2_clean'] = df['relation 2'].apply(lambda x: get_point(x))\n",
    "df['rel_combine'] = df.apply(lambda x: ' '.join([x['rel_1_clean'], x['rel_2_clean']]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "# Assuming you have a DataFrame df with a column 'text' containing text data\n",
    "non_english_rows = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = row['source']\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        # Change 'en' to your desired language code for English\n",
    "        if language != 'en':\n",
    "            non_english_rows.append(index)\n",
    "    except:\n",
    "        # Handle cases where language detection fails (e.g., due to short text)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.index.isin(non_english_rows)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which can be stratified\n",
    "split_dict = dict(df.rel_combine.value_counts() > 1)\n",
    "df['strat_split'] = df.rel_combine.apply(lambda x: split_dict[x])\n",
    "\n",
    "# separate dfs\n",
    "df_str = df[df.strat_split == True].reset_index(drop = True)\n",
    "df_nostr = df[df.strat_split == False].reset_index(drop = True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Specify the feature (topic) column and the target variable (text) column.\n",
    "feature_column = 'rel_combine'\n",
    "text_column = 'result'\n",
    "\n",
    "# Adjust the test_size and random_state as needed.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df_str[text_column],\n",
    "    df_str[feature_column],\n",
    "    test_size=0.2,  # Adjust the test size as needed.\n",
    "    stratify=df_str[feature_column], # stratify by column\n",
    "    random_state=42  # Adjust the random seed as needed.\n",
    ")\n",
    "\n",
    "# Split the remaining data into validation and testing sets (50% each).\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42  # Use the same random seed for consistency.\n",
    ")\n",
    "\n",
    "df_train1 = df_str[df_str.index.isin(X_train.index)]\n",
    "df_val1 = df_str[df_str.index.isin(X_val.index)]\n",
    "df_test1 = df_str[df_str.index.isin(X_test.index)]\n",
    "\n",
    "# Adjust the test_size and random_state as needed.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df_nostr[text_column],\n",
    "    df_nostr[feature_column],\n",
    "    test_size=0.2,  # Adjust the test size as needed.\n",
    "    random_state=42  # Adjust the random seed as needed.\n",
    ")\n",
    "\n",
    "# Split the remaining data into validation and testing sets (50% each).\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42  # Use the same random seed for consistency.\n",
    ")\n",
    "\n",
    "df_train2 = df_nostr[df_nostr.index.isin(X_train.index)]\n",
    "df_val2 = df_nostr[df_nostr.index.isin(X_val.index)]\n",
    "df_test2 = df_nostr[df_nostr.index.isin(X_test.index)]\n",
    "\n",
    "df_train = pd.concat([df_train1,df_train2])\n",
    "df_test = pd.concat([df_test1,df_test2])\n",
    "df_val = pd.concat([df_val1,df_val2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train1,df_train2])\n",
    "df_test = pd.concat([df_test1,df_test2])\n",
    "df_val = pd.concat([df_val1,df_val2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10706, 23)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_json = df_train.reset_index().loc[:1000][['source','target']]\n",
    "val_json = df_val.reset_index().loc[:100][['source','target']]\n",
    "test_json = df_test.reset_index().loc[:100][['source','target']]\n",
    "\n",
    "data_dir = '../data/modelling/'\n",
    "\n",
    "train_file = os.path.join(data_dir,'strain_v1.json')\n",
    "val_file = os.path.join(data_dir,'sval_v1.json')\n",
    "test_file = os.path.join(data_dir,'stest_v1.json')\n",
    "\n",
    "\n",
    "with open(train_file, 'w', encoding = 'utf-8') as train_json_file:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in train_json.iterrows():\n",
    "        # Convert the row to a dictionary and write it to the file\n",
    "        json.dump(row.to_dict(), train_json_file, ensure_ascii = False)\n",
    "        # Add a newline character to separate the JSON objects\n",
    "        train_json_file.write('\\n')\n",
    "\n",
    "with open(val_file, 'w', encoding = 'utf-8') as val_json_file:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in val_json.iterrows():\n",
    "        # Convert the row to a dictionary and write it to the file\n",
    "        json.dump(row.to_dict(), val_json_file, ensure_ascii = False)\n",
    "        # Add a newline character to separate the JSON objects\n",
    "        val_json_file.write('\\n')\n",
    "\n",
    "with open(test_file, 'w', encoding = 'utf-8') as test_json_file:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in test_json.iterrows():\n",
    "        # Convert the row to a dictionary and write it to the file\n",
    "        json.dump(row.to_dict(), test_json_file, ensure_ascii = False)\n",
    "        # Add a newline character to separate the JSON objects\n",
    "        test_json_file.write('\\n')\n",
    "\n",
    "# with open(train_file, 'w') as train_json_file:\n",
    "#     json.dump(train_json, train_json_file)\n",
    "\n",
    "# with open(val_file, 'w') as val_json_file:\n",
    "#     json.dump(val_json, val_json_file)\n",
    "\n",
    "# with open(test_file, 'w') as test_json_file:\n",
    "#     json.dump(test_json, test_json_file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
